{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPK0ftPIs0rTR/grm68LLRY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/Recurrent-Neural-Networks/blob/main/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWasHyrwGtOc"
      },
      "outputs": [],
      "source": [
        "def  model_final (input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.003\n",
        "\n",
        "    # Build the layers    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],input_shape=input_shape[1:]))\n",
        "    model.add(Bidirectional(GRU(128)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,optimizer=Adam(learning_rate),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model_w2v = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
        "model.add(model_w2v.wv.get_keras_embedding(train_embeddings=False))\n",
        "model.add(LSTM(512))\n",
        "model.add(RepeatVector(8))\n",
        "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(LEN_RU, activation='softmax'))"
      ],
      "metadata": {
        "id": "4hhZgXe6igGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "4eFVLmJkisly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "bxwCo0ICiuHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "()preprocessing \n",
        "()embedding layer \n",
        "()rnn \n",
        "()repeatvector \n",
        "()rnn\n",
        "()dense"
      ],
      "metadata": {
        "id": "_mgvAaofix72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hSh2UCJjawK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset ------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "GouMHYKCzLrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Input,Dropout,RepeatVector, LSTM, TimeDistributed,GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import string\n",
        "from numpy import array, argmax, random, take\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import common_texts\n",
        "# from keras_self_attention import SeqSelfAttention"
      ],
      "metadata": {
        "id": "L447S4VciifZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyyD93B7zN5c",
        "outputId": "7eb6371a-72ca-42ad-c2c7-17594e9434cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf /content/drive/MyDrive/French_To_English/fr-en.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMuwvmZw0PF8",
        "outputId": "cbd434f9-f469-4334-b341-580785595d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "europarl-v7.fr-en.en\n",
            "europarl-v7.fr-en.fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load file\n",
        "def load_file(filename):\n",
        "  file=open(filename,'rt',encoding='utf-8')\n",
        "  text=file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "29TPMevB1ekg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Convert Document to sentences\n",
        "def sentenize(text):\n",
        "  sentences=text.strip().split(\"\\n\")\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "jSm-K1KR3oxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shortest and longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        " lengths = [len(s.split()) for s in sentences]\n",
        " return min(lengths), max(lengths)"
      ],
      "metadata": {
        "id": "oAWj7zxQ82cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Sentences\n",
        "def num_sentences(sentences):\n",
        "  return len(sentences)"
      ],
      "metadata": {
        "id": "mYWcd6-Q9n7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Function\n",
        "def preprocess(sentences):\n",
        "\tpreprocessed_sentences = list()\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor sentence in sentences:\n",
        "\t\tsentence = normalize('NFD', sentence).encode('ascii', 'ignore')\n",
        "\t\tsentence = sentence.decode('UTF-8')\n",
        "\t\tsentence = sentence.split()\n",
        "\t\tsentence = [word.lower() for word in sentence]\n",
        "\t\tsentence = [word.translate(table) for word in sentence]\n",
        "\t\tsentence = [re_print.sub('', w) for w in sentence]\n",
        "\t\tsentence = [word for word in sentence if word.isalpha()]\n",
        "\t\tpreprocessed_sentences.append(' '.join(sentence))\n",
        "\treturn preprocessed_sentences"
      ],
      "metadata": {
        "id": "nCHrych85vJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_text=load_file('/content/europarl-v7.fr-en.en')\n",
        "english_sent=sentenize(english_text)\n",
        "english_sent=preprocess(english_sent)"
      ],
      "metadata": {
        "id": "mgUQ6bdP3aUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_text=load_file('/content/europarl-v7.fr-en.fr')\n",
        "french_sent=sentenize(french_text)\n",
        "french_sent=preprocess(french_sent)"
      ],
      "metadata": {
        "id": "EqZJMuar7Zy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimum and Maximum Length in English Sentences\n",
        "eng_minlen,eng_maxlen=sentence_lengths(english_sent)\n",
        "print(\"Minimum English Sentence Length: \",eng_minlen)\n",
        "print(\"Maximum English Sentence Length: \",eng_maxlen)\n",
        "\n",
        "# Minimum and Maximum Length in French Sentences\n",
        "fr_minlen,fr_maxlen=sentence_lengths(french_sent)\n",
        "print(\"Minimum French Sentence Length: \",fr_minlen)\n",
        "print(\"Maximum French Sentence Length: \",fr_maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WJncMyS4Y57",
        "outputId": "33ec05ad-4202-4488-88ec-c516b54768e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum English Sentence Length:  0\n",
            "Maximum English Sentence Length:  642\n",
            "Minimum French Sentence Length:  0\n",
            "Maximum French Sentence Length:  598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Sentences in English Text\n",
        "eng_sen_num=num_sentences(english_sent)\n",
        "print(\"English Text has \" + str(eng_sen_num) + \" sentences\")\n",
        "\n",
        "# Number of Sentences in French Text\n",
        "fr_sen_num=num_sentences(french_sent)\n",
        "print(\"French Text has \" + str(fr_sen_num) + \" sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9zTlLf69zGe",
        "outputId": "0994c875-8cb0-41a6-9162-718c0d13f330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Text has 2007723 sentences\n",
            "French Text has 2007723 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x),tokenizer"
      ],
      "metadata": {
        "id": "epv9X4nfz399"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenized,eng_tokenizer=tokenize(english_sent)\n",
        "fr_tokenized,fr_tokenizer=tokenize(french_sent)"
      ],
      "metadata": {
        "id": "2Zlo-PP50fMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer.word_index\n",
        "fr_tokenizer.word_index "
      ],
      "metadata": {
        "id": "nLEzzHTA2wUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def save(filename,content):\n",
        "  with open(filename, \"wb\") as file:\n",
        "    pickle.dump(content, file)\n",
        "\n",
        "def load(filename):\n",
        "  with open(filename, \"rb\") as file:\n",
        "    content= pickle.load(file)\n",
        "  return content"
      ],
      "metadata": {
        "id": "zuWCRam62n-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save(\"/content/drive/MyDrive/French_To_English/eng_tokenized\",eng_tokenized)\n",
        "save(\"/content/drive/MyDrive/French_To_English/fr_tokenized\",fr_tokenized)\n",
        "save(\"/content/drive/MyDrive/French_To_English/english_sent\",english_sent)\n",
        "save(\"/content/drive/MyDrive/French_To_English/french_sent\",french_sent)\n",
        "save(\"/content/drive/MyDrive/French_To_English/eng_tokenizer\",eng_tokenizer)\n",
        "save(\"/content/drive/MyDrive/French_To_English/fr_tokenizer\",fr_tokenizer)\n",
        "save(\"/content/drive/MyDrive/French_To_English/eng_padded\",eng_padded)\n",
        "save(\"/content/drive/MyDrive/French_To_English/fr_padded\",fr_padded)"
      ],
      "metadata": {
        "id": "3kMk71_CthjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenized=load(\"/content/drive/MyDrive/French_To_English/eng_tokenized\")\n",
        "fr_tokenized=load(\"/content/drive/MyDrive/French_To_English/fr_tokenized\")\n",
        "english_sent=load(\"/content/drive/MyDrive/French_To_English/english_sent\")\n",
        "french_sent=load(\"/content/drive/MyDrive/French_To_English/french_sent\")\n",
        "eng_tokenizer=load(\"/content/drive/MyDrive/French_To_English/eng_tokenizer\")\n",
        "fr_tokenizer=load(\"/content/drive/MyDrive/French_To_English/fr_tokenizer\")\n",
        "eng_padded=load(\"/content/drive/MyDrive/French_To_English/eng_padded\")\n",
        "fr_padded=load(\"/content/drive/MyDrive/French_To_English/fr_padded\")"
      ],
      "metadata": {
        "id": "oqSRo8T2tj2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "eng_tokenized=np.array(eng_tokenized)\n",
        "fr_tokenized=np.array(fr_tokenized)"
      ],
      "metadata": {
        "id": "HbHAtXhRL8Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(sequences,length=None):\n",
        "  padded=pad_sequences(sequences,maxlen=length,padding=\"post\")\n",
        "  return padded"
      ],
      "metadata": {
        "id": "0FkYGAK33Utu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_padded=pad(eng_tokenized,150)\n",
        "fr_padded=pad(fr_tokenized,150)"
      ],
      "metadata": {
        "id": "six6n4ed4srH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab_size=len(eng_tokenizer.word_index)\n",
        "french_vocab_size=len(fr_tokenizer.word_index)\n",
        "print(\"English Dictionary Size: \",english_vocab_size)\n",
        "print(\"French Dictionary Size: \",french_vocab_size)"
      ],
      "metadata": {
        "id": "Mu4Zf8M0bP3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4cf255-162d-4815-f9e7-c759fa366460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Dictionary Size:  105357\n",
            "French Dictionary Size:  141642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "metadata": {
        "id": "K9-VZUza-jrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a basic RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tests.test_simple_model(simple_model)\n",
        "\n",
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "# Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "\n",
        "print(simple_rnn_model.summary())\n",
        "\n",
        "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "metadata": {
        "id": "qr48uqVhBfQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(x, y):\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sent, french_sent)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "metadata": {
        "id": "FFvDqzkxEdxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Function\n",
        "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "      model.add(LSTM(units))\n",
        "      model.add(RepeatVector(out_timesteps))\n",
        "      model.add(LSTM(units, return_sequences=True))\n",
        "      model.add(Dense(out_vocab, activation='softmax'))\n",
        "      model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0004), \n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "      return model"
      ],
      "metadata": {
        "id": "75APbUUOW1eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=define_model(french_vocab_size, english_vocab_size, 50, 50, 1024)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ucbA6jYAXoKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f529220d-4fa6-4de7-a7da-e6f607c7ae65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 50, 1024)          145041408 \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 1024)              8392704   \n",
            "                                                                 \n",
            " repeat_vector_5 (RepeatVect  (None, 50, 1024)         0         \n",
            " or)                                                             \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 50, 1024)          8392704   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 50, 105357)        107990925 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 269,817,741\n",
            "Trainable params: 269,817,741\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(eng_padded, fr_padded.reshape(fr_padded.shape[0], fr_padded.shape[1], 1),\n",
        "                    epochs=10, \n",
        "                    batch_size=64, \n",
        "                    steps_per_epoch=32,\n",
        "                    # callbacks=[checkpoint], \n",
        "                    # verbose=0,\n",
        "                    use_multiprocessing=True\n",
        "          )"
      ],
      "metadata": {
        "id": "qpTXOkuCmRM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afbce475-26f4-445a-a414-81ad8aa936fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 34s 879ms/step - loss: nan - accuracy: 0.4927\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 28s 869ms/step - loss: nan - accuracy: 0.5019\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 28s 870ms/step - loss: nan - accuracy: 0.5030\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 27s 847ms/step - loss: nan - accuracy: 0.4974\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 26s 828ms/step - loss: nan - accuracy: 0.5057\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 27s 844ms/step - loss: nan - accuracy: 0.4972\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 26s 812ms/step - loss: nan - accuracy: 0.5087\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 26s 816ms/step - loss: nan - accuracy: 0.4985\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 26s 795ms/step - loss: nan - accuracy: 0.5053\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 26s 816ms/step - loss: nan - accuracy: 0.4963\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc228e755d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxtdrB7ImmmA",
        "outputId": "3b4af660-d55c-4bbe-eb7d-f314a84c6313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.39.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.10/dist-packages (from numba) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba) (67.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda \n",
        "device = cuda.get_current_device()\n",
        "device.reset()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AhYbDCSmooW",
        "outputId": "a44b0374-9e32-4b6f-9553-4d41f48aa414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}