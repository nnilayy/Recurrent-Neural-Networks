{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9W/Orz7Hg1AR2R8wX1kIg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/Recurrent-Neural-Networks/blob/main/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWasHyrwGtOc"
      },
      "outputs": [],
      "source": [
        "def  model_final (input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.003\n",
        "\n",
        "    # Build the layers    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],input_shape=input_shape[1:]))\n",
        "    model.add(Bidirectional(GRU(128)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,optimizer=Adam(learning_rate),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import string\n",
        "from numpy import array, argmax, random, take\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import common_texts\n",
        "from keras_self_attention import SeqSelfAttention"
      ],
      "metadata": {
        "id": "L447S4VciifZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model_w2v = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
        "model.add(model_w2v.wv.get_keras_embedding(train_embeddings=False))\n",
        "model.add(LSTM(512))\n",
        "model.add(RepeatVector(8))\n",
        "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(LEN_RU, activation='softmax'))"
      ],
      "metadata": {
        "id": "4hhZgXe6igGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "4eFVLmJkisly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "bxwCo0ICiuHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "()preprocessing \n",
        "()embedding layer \n",
        "()rnn \n",
        "()repeatvector \n",
        "()rnn\n",
        "()dense"
      ],
      "metadata": {
        "id": "_mgvAaofix72"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}