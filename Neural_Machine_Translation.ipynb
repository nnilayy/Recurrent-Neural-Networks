{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/J7tCwsheWZHJ5439O+HL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnilayy/Recurrent-Neural-Networks/blob/main/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PWasHyrwGtOc"
      },
      "outputs": [],
      "source": [
        "def  model_final (input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.003\n",
        "\n",
        "    # Build the layers    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],input_shape=input_shape[1:]))\n",
        "    model.add(Bidirectional(GRU(128)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,optimizer=Adam(learning_rate),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import string\n",
        "from numpy import array, argmax, random, take\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import common_texts\n",
        "# from keras_self_attention import SeqSelfAttention"
      ],
      "metadata": {
        "id": "L447S4VciifZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model_w2v = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
        "model.add(model_w2v.wv.get_keras_embedding(train_embeddings=False))\n",
        "model.add(LSTM(512))\n",
        "model.add(RepeatVector(8))\n",
        "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(LEN_RU, activation='softmax'))"
      ],
      "metadata": {
        "id": "4hhZgXe6igGp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "fdf84680-91bc-4cb2-a476-4fcb9cd696c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2a2a1229f6a4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Word2Vec.__init__() got an unexpected keyword argument 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "4eFVLmJkisly"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "bxwCo0ICiuHr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "6cfff2ca-c8bd-4aba-c108-f106270435ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5f15418b3570>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \"\"\"\n\u001b[1;32m   3228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3229\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3230\u001b[0m                 \u001b[0;34m\"This model has not yet been built. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m                 \u001b[0;34m\"Build the model first by calling `build()` or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "()preprocessing \n",
        "()embedding layer \n",
        "()rnn \n",
        "()repeatvector \n",
        "()rnn\n",
        "()dense"
      ],
      "metadata": {
        "id": "_mgvAaofix72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hSh2UCJjawK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset ------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "GouMHYKCzLrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize"
      ],
      "metadata": {
        "id": "FK_5m-wS6vSi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyyD93B7zN5c",
        "outputId": "760c93f3-ffcc-4eab-f716-4133113697f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf /content/drive/MyDrive/French_To_English/fr-en.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMuwvmZw0PF8",
        "outputId": "dcbc6fca-c5b2-4d77-ea45-28b6511ffda1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "europarl-v7.fr-en.en\n",
            "europarl-v7.fr-en.fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=\"hi babe how are you\"\n",
        "data.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or2MGIj_01nM",
        "outputId": "d0806b8e-8985-45bc-884b-655bc826613b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'babe', 'how', 'are', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load file\n",
        "def load_file(filename):\n",
        "  file=open(filename,'rt',encoding='utf-8')\n",
        "  text=file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "29TPMevB1ekg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Convert Document to sentences\n",
        "def sentenize(text):\n",
        "  sentences=text.strip().split(\"\\n\")\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "jSm-K1KR3oxM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shortest and longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        " lengths = [len(s.split()) for s in sentences]\n",
        " return min(lengths), max(lengths)"
      ],
      "metadata": {
        "id": "oAWj7zxQ82cS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Sentences\n",
        "def num_sentences(sentences):\n",
        "  return len(sentences)"
      ],
      "metadata": {
        "id": "mYWcd6-Q9n7f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Function\n",
        "def preprocess(sentences):\n",
        "\tpreprocessed_sentences = list()\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor sentence in sentences:\n",
        "\t\tsentence = normalize('NFD', sentence).encode('ascii', 'ignore')\n",
        "\t\tsentence = sentence.decode('UTF-8')\n",
        "\t\tsentence = sentence.split()\n",
        "\t\tsentence = [word.lower() for word in sentence]\n",
        "\t\tsentence = [word.translate(table) for word in sentence]\n",
        "\t\tsentence = [re_print.sub('', w) for w in sentence]\n",
        "\t\tsentence = [word for word in sentence if word.isalpha()]\n",
        "\t\tpreprocessed_sentences.append(' '.join(sentence))\n",
        "\treturn preprocessed_sentences"
      ],
      "metadata": {
        "id": "nCHrych85vJo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_text=load_file('/content/europarl-v7.fr-en.en')\n",
        "english_sent=sentenize(english_text)\n",
        "english_sent=preprocess(english_sent)"
      ],
      "metadata": {
        "id": "mgUQ6bdP3aUO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_text=load_file('/content/europarl-v7.fr-en.fr')\n",
        "french_sent=sentenize(french_text)\n",
        "french_sent=preprocess(french_sent)"
      ],
      "metadata": {
        "id": "EqZJMuar7Zy9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimum and Maximum Length in English Sentences\n",
        "eng_minlen,eng_maxlen=sentence_lengths(english_sent)\n",
        "print(\"Minimum English Sentence Length: \",eng_minlen)\n",
        "print(\"Maximum English Sentence Length: \",eng_maxlen)\n",
        "\n",
        "# Minimum and Maximum Length in French Sentences\n",
        "fr_minlen,fr_maxlen=sentence_lengths(french_sent)\n",
        "print(\"Minimum French Sentence Length: \",fr_minlen)\n",
        "print(\"Maximum French Sentence Length: \",fr_maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WJncMyS4Y57",
        "outputId": "3283d8e7-24b4-4e65-de80-dea574199b49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum English Sentence Length:  0\n",
            "Maximum English Sentence Length:  642\n",
            "Minimum French Sentence Length:  0\n",
            "Maximum French Sentence Length:  598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Sentences in English Text\n",
        "eng_sen_num=num_sentences(english_sent)\n",
        "print(\"English Text has \" + str(eng_sen_num) + \" sentences\")\n",
        "\n",
        "# Number of Sentences in French Text\n",
        "fr_sen_num=num_sentences(french_sent)\n",
        "print(\"French Text has \" + str(fr_sen_num) + \" sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9zTlLf69zGe",
        "outputId": "076d90a7-2105-491f-b8df-1cd0538282b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Text has 2007723 sentences\n",
            "French Text has 2007723 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x),tokenizer"
      ],
      "metadata": {
        "id": "epv9X4nfz399"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenized,eng_tokenizer=tokenize(english_sent)\n",
        "fr_tokenized,fr_tokenizer=tokenize(french_sent)"
      ],
      "metadata": {
        "id": "2Zlo-PP50fMK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer.word_index\n",
        "fr_tokenizer.word_index"
      ],
      "metadata": {
        "id": "nLEzzHTA2wUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def save(filename,content):\n",
        "  with open(filename, \"wb\") as file:\n",
        "    pickle.dump(content, file)\n",
        "\n",
        "def load(filename):\n",
        "  with open(filename, \"rb\") as file:\n",
        "    content= pickle.load(file)\n",
        "  return content"
      ],
      "metadata": {
        "id": "zuWCRam62n-u"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save(\"/content/drive/MyDrive/French_To_English/eng_tokenized\",eng_tokenized)\n",
        "save(\"/content/drive/MyDrive/French_To_English/fr_tokenized\",fr_tokenized)"
      ],
      "metadata": {
        "id": "9x5ux_vt8t2z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenized=load(\"/content/drive/MyDrive/French_To_English/eng_tokenized\")\n",
        "fr_tokenized=load(\"/content/drive/MyDrive/French_To_English/fr_tokenized\")"
      ],
      "metadata": {
        "id": "eqfToEGQ9WLj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(sequences,length=None):\n",
        "  padded=pad_sequences(sequences,maxlen=length,padding=\"post\")\n",
        "  return padded"
      ],
      "metadata": {
        "id": "0FkYGAK33Utu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_padded=pad(eng_tokenized,200)\n",
        "fr_padded=pad(fr_tokenized,200)"
      ],
      "metadata": {
        "id": "six6n4ed4srH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9-VZUza-jrs",
        "outputId": "dc8e1f15-e4a6-45aa-c970-81f95422f878"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4733,    2,    1, 1574,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}